{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8107d398",
   "metadata": {},
   "source": [
    "# 평가 조건\n",
    "1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.\n",
    "    - klue/bert-base를 pretrained weight 없이 불러와 토큰화된 NSMC 데이터셋을 입력하여 훈련시킬 때, 모델이 정상적으로 작동하는 것을 확인하였다.\n",
    "2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.\n",
    "    - Validation accuracy를 90% 이상으로 개선하였다.\n",
    "3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.\n",
    "    - Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59f181",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "717edf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257e932",
   "metadata": {},
   "source": [
    "# 1. NSMC 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbbd7341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b74468d25e24129bd4d8ab45a6d0062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_dataset = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c62d833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e82268",
   "metadata": {},
   "source": [
    "데이터는 총 200000만개로 train 15만개, test 5만개이며, id, document, label로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ed31b",
   "metadata": {},
   "source": [
    "## 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df140470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = row_dataset['train']\n",
    "cols = train.column_names\n",
    "\n",
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf879c",
   "metadata": {},
   "source": [
    "## 데이터 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d758a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최소 길이 :0\n",
      "리뷰의 최대 길이 :146\n",
      "리뷰의 평균 길이 :35.20335333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최소 길이 :{}'.format(min(len(l) for l in train['document'])))\n",
    "print('리뷰의 최대 길이 :{}'.format(max(len(l) for l in train['document'])))\n",
    "print('리뷰의 평균 길이 :{}'.format(sum(map(len, train['document']))/len(train['document'])))\n",
    "\n",
    "plt.hist([len(s) for s in train['document']], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2467a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25857\n",
      "55737\n",
      "110014\n",
      "126782\n",
      "140721\n"
     ]
    }
   ],
   "source": [
    "for l, v in enumerate(train['document']):\n",
    "    if len(v) == 0:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e5bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['document'][25857]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96378cc1",
   "metadata": {},
   "source": [
    "길이가 0인 데이터는 아무것도 없는 데이터인것을 확인하였습니다.\n",
    "\n",
    "해당 데이터는 추후 전처리에서 제거해주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287b96f",
   "metadata": {},
   "source": [
    "## 타겟 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e474e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([75173, 74827])\n"
     ]
    }
   ],
   "source": [
    "result = Counter(train['label']).values()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c45fb7",
   "metadata": {},
   "source": [
    "0(부정)인 데이터가 75,173개, 1(긍정)인 데이터가 74,827개가 있으므로 밸런스한 데이터인것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b1139",
   "metadata": {},
   "source": [
    "# 2. huggingface에서 model 및 tokenizer 불러오기\n",
    "huggingface에서 사전학습된 모델을 불러옵니다.\n",
    "\n",
    "저는 klue/bert 모델과 klue/roberta 모델을 불러와서 두 모델의 성능을 비교해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1017e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "355d9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
      "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ce0a8",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리\n",
    "길이가 0인 데이터를 제거해주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3115534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-b626d26578c9f24a.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-f1144a1c5e8f9fce.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 149995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 49997\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_dataset = row_dataset.filter(lambda x: len(x['document']) > 0)\n",
    "row_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b951a13",
   "metadata": {},
   "source": [
    "최종 train data는 149,995개 / test data는 49,997개 입니다.\n",
    "\n",
    "다만 해당 데이터를 모두 사용하면 학습과 추론에 상당한 시간이 걸리기 때문에 train 50,000개 / test 10,000개를 추출하여 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829c4488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc9cebf5fa7440baa3d6f4c0fd64aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7694f48d5af4c32a9ebce6083228692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bert_transform(data):\n",
    "    return bert_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )\n",
    "\n",
    "bert_row_dataset = row_dataset.map(bert_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5be36220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-f84ed4a324e82307.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920916af5eff4c2b8c68cc5e1149b8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def roberta_transform(data):\n",
    "    return roberta_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )\n",
    "\n",
    "roberta_row_dataset = row_dataset.map(roberta_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f137a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataset = bert_row_dataset['train'].shuffle(random_seed).select(range(50000))\n",
    "bert_test_dataset = bert_row_dataset['test'].shuffle(random_seed).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1610e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataset, bert_val_dataset= bert_train_dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd18171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998c2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_train_dataset = roberta_row_dataset['train'].shuffle(random_seed).select(range(50000))\n",
    "roberta_test_dataset = roberta_row_dataset['test'].shuffle(random_seed).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23c1e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_train_dataset, roberta_val_dataset= roberta_train_dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810d6f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f0b31",
   "metadata": {},
   "source": [
    "# 4. model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faaf3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/going_deeper/model'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d92d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9cf2425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"accuracy\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions: Predicted labels, as returned by a model.\n",
       "    references: Ground truth labels.\n",
       "    normalize: If False, return the number of correctly classified samples.\n",
       "        Otherwise, return the fraction of correctly classified samples.\n",
       "    sample_weight: Sample weights.\n",
       "Returns:\n",
       "    accuracy: Accuracy score.\n",
       "Examples:\n",
       "\n",
       "    >>> accuracy_metric = datasets.load_metric(\"accuracy\")\n",
       "    >>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372c348",
   "metadata": {},
   "source": [
    "## 4-1. bert model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37602dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 3:40:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.325958</td>\n",
       "      <td>0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.398353</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.545082</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.24248295389811197, metrics={'train_runtime': 13219.5427, 'train_samples_per_second': 9.077, 'train_steps_per_second': 1.135, 'total_flos': 3.15733266432e+16, 'train_loss': 0.24248295389811197, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_trainer = Trainer(\n",
    "    model=bert_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=bert_train_dataset,    # training dataset\n",
    "    eval_dataset=bert_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d23d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 06:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5151875019073486,\n",
       " 'eval_accuracy': 0.8946,\n",
       " 'eval_runtime': 389.0046,\n",
       " 'eval_samples_per_second': 25.707,\n",
       " 'eval_steps_per_second': 3.213,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_trainer.evaluate(bert_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746206c6",
   "metadata": {},
   "source": [
    "## 4-2. roberta 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "886d02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 3:46:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.336500</td>\n",
       "      <td>0.290169</td>\n",
       "      <td>0.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.394025</td>\n",
       "      <td>0.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.457633</td>\n",
       "      <td>0.895600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.275982410176595, metrics={'train_runtime': 13575.9804, 'train_samples_per_second': 8.839, 'train_steps_per_second': 1.105, 'total_flos': 3.15733266432e+16, 'train_loss': 0.275982410176595, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=roberta_train_dataset,    # training dataset\n",
    "    eval_dataset=roberta_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "roberta_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9975aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 06:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.44005903601646423,\n",
       " 'eval_accuracy': 0.8983,\n",
       " 'eval_runtime': 382.0675,\n",
       " 'eval_samples_per_second': 26.173,\n",
       " 'eval_steps_per_second': 3.272,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer.evaluate(roberta_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5eb2b6",
   "metadata": {},
   "source": [
    "## 4-3. 추가적인 성능 향상 시도\n",
    "BERT모델과 ROBERTA 모델 중 ROBERTA의 성능이 더 좋아 해당 모델을 이용하여 추가적인 성능 향상 시도를 해보겠습니다.\n",
    "1. data_collator 추가 \n",
    "2. 파라미터 변경 및 추가\n",
    "    - warmup_ratio 변수 추가\n",
    "    - group_by_length 변수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4f3afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4360e7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bba3a15b5d543158235df00eb816792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33be45544da1473b8bc71ce8f17b59d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def roberta_transform(data):\n",
    "    return roberta_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )\n",
    "\n",
    "roberta_row_dataset = row_dataset.map(roberta_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef3e26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_train_dataset = roberta_row_dataset['train'].shuffle(random_seed).select(range(50000))\n",
    "roberta_test_dataset = roberta_row_dataset['test'].shuffle(random_seed).select(range(10000))\n",
    "\n",
    "roberta_train_dataset, roberta_val_dataset= roberta_train_dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b127da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/going_deeper/model'\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    "    warmup_ratio = 0.1,\n",
    "    group_by_length = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c571aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    m1 = load_metric('accuracy')\n",
    "    m2 = load_metric('f1')\n",
    "\n",
    "    acc = m1.compute(predictions=preds, references=labels)['accuracy']\n",
    "    f1 = m2.compute(predictions=preds, references=labels)['f1']\n",
    "\n",
    "    return {'accuracy':acc, 'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31de7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 3:41:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.348361</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.879975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.396007</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.893651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.458208</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.896290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/going_deeper/model/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/going_deeper/model/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.2873779469807943, metrics={'train_runtime': 13309.4524, 'train_samples_per_second': 9.016, 'train_steps_per_second': 1.127, 'total_flos': 3.15733266432e+16, 'train_loss': 0.2873779469807943, 'epoch': 3.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=roberta_train_dataset,    # training dataset\n",
    "    eval_dataset=roberta_val_dataset,       # evaluation dataset\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "roberta_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b674ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 05:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.42159461975097656,\n",
       " 'eval_accuracy': 0.9017,\n",
       " 'eval_f1': 0.9029902299417744,\n",
       " 'eval_runtime': 340.9479,\n",
       " 'eval_samples_per_second': 29.33,\n",
       " 'eval_steps_per_second': 3.666,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer.evaluate(roberta_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6564938",
   "metadata": {},
   "source": [
    "# 5. 결과 비교\n",
    "|  model  |data_collator|group_by_length|   time   |val accuracy|val F1-Score|test accuracy|test F1-Score|\n",
    "|---------|-------------|---------------|----------|------------|------------|-------------|-------------|\n",
    "|   BERT  |      x      |       x       | 3:40:18  |    88.93   |      -     |    89.46    |      -      |\n",
    "|RoBERTa_1|      x      |       x       | 3:46:14  |    89.56   |      -     |    89.83    |      -      |\n",
    "|RoBERTa_2|      o      |       o       | 3:41:48  |    89.63   |   89.63    | __90.17__  |__90.30__   |\n",
    "\n",
    "Hugging Face를 이용하여 BERT, RoBERTa 모델과 토크나이저를 불러와 NSMC 데이터를 학습하고 해당 모델의 성능을 비교하였습니다. BERT와 RoBERTa는 동일한 파라미터를 통하여 실험을 하였으며, RoBERTa의 성능이 BERT보다 좋다는 것을 확인하였습니다. 그 후 RoBERTa를 이용하여 추가적인 성능향상을 시도하였습니다. data_collator와 group_by_length 변수를 활용하였는데, 해당 변수를 사용하였을때 시간을 줄어들고, 성능은 약간 개선되는 것을 확인할 수 있었습니다. data_collator와 group_by_length 데이터들 간의 길이를 이용하여 데이터를 모델에 효율적으로 제공하는 역할을 하기 때문에 성능과 속도 측면에서 개선된것으로 생각됩니다. 또한 모든 데이터를 사용하여 실험을 했다면, 성능은 더 올랐을것으로 생각됩니다.\n",
    "\n",
    "group_by_length\n",
    "- 데이터 배치를 길이에 따라 자동으로 그룹화하여 데이터 로딩 및 배치 생성 과정에서 각 배치의 텍스트 시퀀스 길이가 유사한 데이터끼리 그룹으로 묶이도록 하는 기능을 담당\n",
    "\n",
    "장점\n",
    "1. 메모리 효율성: 텍스트 시퀀스의 길이가 다양한 경우, 길이가 유사한 데이터끼리 배치를 구성하면 메모리를 더 효율적으로 사용할 수 있습니다. 길이가 유사한 데이터끼리 배치를 구성하면 각 배치의 패딩이 최소화되어 메모리 사용량이 줄어들고, 학습 속도가 향상될 수 있습니다.\n",
    "\n",
    "2. 학습 효율성: 길이가 유사한 데이터끼리 배치를 구성하면 학습 속도가 향상될 수 있습니다. 각 배치의 텍스트 시퀀스 길이가 유사하면 GPU에서 병렬로 연산을 수행하는 데 효율적이며, 데이터 로딩과 배치 생성 시간이 줄어들어 전체 학습 시간을 단축시킬 수 있습니다.\n",
    "\n",
    "3. 학습 안정성: 길이가 크게 다른 데이터를 섞어서 배치를 구성하면 학습이 불안정해질 수 있습니다. 길이가 다른 데이터를 함께 배치로 구성하면 작은 시퀀스가 긴 시퀀스에 의해 연산이 지연될 수 있고, 학습이 불균형하게 진행될 수 있습니다. 이에 반해 길이가 유사한 데이터끼리 배치를 구성하면 학습이 안정적으로 진행될 가능성이 높아집니다.\n",
    "\n",
    "\n",
    "Data Collator\n",
    "- Data Collator는 데이터셋에서 가져온 샘플들을 그룹화하고 전처리하는 기능을 담당\n",
    "\n",
    "장점\n",
    "1. 배치 처리: Data Collator는 데이터셋으로부터 미니배치(mini-batch)를 생성하는 역할을 합니다. 데이터를 모델에 효율적으로 공급하기 위해 여러 샘플을 하나의 배치로 묶어주는 작업을 담당합니다. 이를 통해 GPU 또는 TPU와 같은 가속화 장치를 최대한 활용하여 병렬 처리를 수행할 수 있습니다.\n",
    "\n",
    "2. 패딩 처리: Data Collator는 데이터셋의 시퀀스를 패딩하여 일괄적인 길이로 맞춰줍니다. 시퀀스 길이가 다른 데이터를 배치로 처리하려면 길이가 가장 큰 시퀀스에 맞춰서 패딩을 추가해야 합니다. Data Collator는 이러한 패딩 작업을 자동으로 처리하여 배치 내 모든 시퀀스의 길이를 동일하게 맞출 수 있습니다.\n",
    "\n",
    "3. 특수 토큰 추가: Data Collator는 특수 토큰(예: 문장 시작, 문장 종료, 패딩)을 시퀀스에 추가하는 작업을 수행합니다. 이러한 특수 토큰은 모델에게 문장 구조를 인식하고 처리하는 데 도움을 줍니다.\n",
    "\n",
    "4. 사용자 정의 처리: Data Collator를 사용하면 사용자가 원하는 대로 데이터를 처리할 수 있습니다. 예를 들어, 어휘 사전을 기반으로 정수 인코딩을 수행하거나, 토큰화된 데이터를 인코딩할 때 사용자가 추가적인 전처리를 수행할 수 있습니다.\n",
    "\n",
    "Data Collator의 이점은 데이터를 효율적으로 처리하고 모델 학습에 필요한 데이터 배치를 생성하는 데 있습니다. 이를 통해 학습 과정의 속도와 안정성을 향상시킬 수 있고, 모델의 성능을 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444c53d",
   "metadata": {},
   "source": [
    "# 전체 회고\n",
    "- 허깅페이스를 오랜만에 다뤄본것 같아 재밌었습니다.\n",
    "- exploration 당시에도 해당 데이터를 이용하여 딥러닝으로 test set 기준 accuracy 85%를 넘었지만, 이번 과제에서는 사전학습 모델을 이용하여 90%를 넘었습니다.\n",
    "- 사전학습 모델의 강력함을 다시 한번 느낀 계기가 되었습니다. \n",
    "- 아이펠톤이라고 생각하고 이것저것 많이 찾아보고 시도해보고자 하였습니다.\n",
    "- 하지만 학습을 시키는데 시간이 많이 걸려서 여러가지 실험을 많이 못해본것이 아쉽습니다.\n",
    "- 그동안 진행했던 과제들을 다시 한번 살펴보면서 아이펠톤을 준비해야겠다는 생각이 많이 들었습니다.\n",
    "- 잘 준비해서 앞으로 있을 과제를 잘 마무리하고 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586eb31",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d66d6",
   "metadata": {},
   "source": [
    "- https://huggingface.co/klue/bert-base\n",
    "- https://huggingface.co/klue/roberta-base\n",
    "- 이전 노드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
