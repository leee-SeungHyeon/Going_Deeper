{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddd04c3",
   "metadata": {},
   "source": [
    "# 평가문항\n",
    "1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.\n",
    "    - MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.\n",
    "    - 학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.\n",
    "    - 학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b985ed",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b62b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3474ee2",
   "metadata": {},
   "source": [
    "# 1. 토크나이저 만들기\n",
    "kowiki.txt 데이터를 이용하여 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20ecf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/data/kowiki.txt --model_prefix=ko_8000 --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n"
     ]
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e62a2b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = os.getenv('HOME')+'/data'\n",
    "\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0f92782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[BOS]', '[EOS]', '[SEP]', '[CLS]', '[MASK]', '▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아', '▁하', '▁있다', '▁다', '▁제']\n"
     ]
    }
   ],
   "source": [
    "vocab_list = []\n",
    "for id in range(len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd8446",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리\n",
    "## 2-1. MASK 생성\n",
    "BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0680bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK] /만약 dice 값이 0.8보다 작으면, 80%의 확률로 masked_token을 \"[MASK]\"로 설정\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original / dice 값이 0.8보다 크고 0.9보다 작으면, 10%의 확률로 masked_token을 해당 토큰의 원래 값인 tokens[index]로 설정\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560b4d0",
   "metadata": {},
   "source": [
    "## 3-2. NSP pair 생성\n",
    "BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.\n",
    "\n",
    "두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.\n",
    "\n",
    "MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1147467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0] #tokens_a의 길이가 tokens_b의 길이보다 크다면, tokens_a의 첫 번째 토큰을 제거\n",
    "        else:\n",
    "            tokens_b.pop() #tokens_b의 길이가 tokens_a의 길이 이상인 경우, tokens_b의 마지막 토큰을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e81aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87109df8",
   "metadata": {},
   "source": [
    "## 3-3. 데이터셋 완성\n",
    "BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0906d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c11cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306d556022654f72ba5df38ee3890b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/going_deeper/data/kowiki.txt'\n",
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/going_deeper/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfbb6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c726293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bce95cf4234ae082d186d7abd4cab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_108/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_108/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01e44c",
   "metadata": {},
   "source": [
    "## 3-4. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c06c824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([[   5,   10, 1605, ...,   16, 3599,    4],\n",
       "         [   5,    6,   37, ...,   43, 3599,    4],\n",
       "         [   5, 3604, 2432, ..., 1853, 3599,    4],\n",
       "         ...,\n",
       "         [   5,  146, 3646, ...,    0,    0,    0],\n",
       "         [   5,  557, 4375, ...,    0,    0,    0],\n",
       "         [   5, 3676,  848, ..., 3637, 2263,    4]], dtype=int32),\n",
       " memmap([[0, 0, 0, ..., 1, 1, 1],\n",
       "         [0, 0, 0, ..., 1, 1, 1],\n",
       "         [0, 0, 0, ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 1, 1, 1]], dtype=int32))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa46f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 1, 1, ..., 1, 1, 1], dtype=int32),\n",
       " memmap([[   0,    0,    0, ...,    0,    0,    0],\n",
       "         [   0, 3630,    0, ...,    0,    0,    0],\n",
       "         [   0,    0,    0, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   0,  146, 3646, ...,    0,    0,    0],\n",
       "         [   0,    0,    0, ...,    0,    0,    0],\n",
       "         [   0,    0,    0, ...,    0,    0,    0]], dtype=int32))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4b0d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = (pre_train_inputs[0][:100], pre_train_inputs[1][:100])\n",
    "test_label = (pre_train_labels[0][:100], pre_train_labels[1][:100])\n",
    "\n",
    "pre_train_inputs = (pre_train_inputs[0][100:], pre_train_inputs[1][100:])\n",
    "pre_train_labels = (pre_train_labels[0][100:], pre_train_labels[1][100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "245795b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127900"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_train_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b7ed3",
   "metadata": {},
   "source": [
    "# 4. BERT 모델 구현\n",
    "pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.\n",
    "\n",
    "Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ce3b3",
   "metadata": {},
   "source": [
    "## 4-1. 함수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c21bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94573511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da88d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a463e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9884df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45c107df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc1a747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "113b5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a09adba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3fb47651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4d8f96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 512,\n",
       " 'n_head': 6,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 4,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 512, \"n_head\": 6, \"d_head\": 64, \"dropout\": 0.1,\n",
    "                 \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 4, \n",
    "                 \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f56f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 512), (None, 11593728    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            263680      bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 11,857,408\n",
      "Trainable params: 11,857,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f486d1",
   "metadata": {},
   "source": [
    "# 5. pretrain 진행\n",
    "loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98e1f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6efe1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e72c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b9f0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 19990\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7479d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1999/1999 [==============================] - 507s 250ms/step - loss: 18.7539 - nsp_loss: 0.6422 - mlm_loss: 18.1117 - nsp_acc: 0.5972 - mlm_lm_acc: 0.1246\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.12455, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "1999/1999 [==============================] - 499s 250ms/step - loss: 15.4106 - nsp_loss: 0.6175 - mlm_loss: 14.7931 - nsp_acc: 0.6186 - mlm_lm_acc: 0.1645\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.12455 to 0.16452, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "1999/1999 [==============================] - 499s 250ms/step - loss: 13.2100 - nsp_loss: 0.6091 - mlm_loss: 12.6010 - nsp_acc: 0.6267 - mlm_lm_acc: 0.2195\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.16452 to 0.21951, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "1999/1999 [==============================] - 499s 250ms/step - loss: 12.3364 - nsp_loss: 0.6050 - mlm_loss: 11.7315 - nsp_acc: 0.6361 - mlm_lm_acc: 0.2502\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.21951 to 0.25022, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "1999/1999 [==============================] - 500s 250ms/step - loss: 11.7265 - nsp_loss: 0.6001 - mlm_loss: 11.1264 - nsp_acc: 0.6458 - mlm_lm_acc: 0.2723\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.25022 to 0.27227, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "1999/1999 [==============================] - 500s 250ms/step - loss: 11.2348 - nsp_loss: 0.5945 - mlm_loss: 10.6403 - nsp_acc: 0.6580 - mlm_lm_acc: 0.2908\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.27227 to 0.29077, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "1999/1999 [==============================] - 500s 250ms/step - loss: 10.8230 - nsp_loss: 0.5853 - mlm_loss: 10.2377 - nsp_acc: 0.6755 - mlm_lm_acc: 0.3065\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.29077 to 0.30650, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "1999/1999 [==============================] - 499s 250ms/step - loss: 10.4911 - nsp_loss: 0.5733 - mlm_loss: 9.9178 - nsp_acc: 0.6977 - mlm_lm_acc: 0.3199\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.30650 to 0.31986, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "1999/1999 [==============================] - 498s 249ms/step - loss: 10.2622 - nsp_loss: 0.5601 - mlm_loss: 9.7021 - nsp_acc: 0.7186 - mlm_lm_acc: 0.3291\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.31986 to 0.32909, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "1999/1999 [==============================] - 499s 250ms/step - loss: 10.1461 - nsp_loss: 0.5519 - mlm_loss: 9.5942 - nsp_acc: 0.7313 - mlm_lm_acc: 0.3338\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.32909 to 0.33381, saving model to /aiffel/aiffel/going_deeper/data/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "save_path = os.getenv('HOME')+'/aiffel/going_deeper/data'\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{save_path}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948a76c",
   "metadata": {},
   "source": [
    "# 6. 훈련 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9f67217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250277e",
   "metadata": {},
   "source": [
    "# 7. 테스트 데이터 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11504f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다.[MASK][MASK][MASK]鬼 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을[MASK][MASK][MASK][MASK][MASK][MASK] \"땅콩 농부\" (Peanut Farmer)로 알려졌다.[SEP][MASK][MASK] 카터[MASK] 얼 \"지미\"[MASK][MASK] 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국[MASK][MASK][MASK] 대통령 (1977년 ~ 1981년)이다.[SEP]'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#인풋데이터\n",
    "vocab.decode_ids(test_input[0][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "910bb0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#세그먼트\n",
    "test_input[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d0911a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nsp label\n",
    "test_label[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f1b40f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1953년', '미국', '벌었다.', '그의', '별명이', '지미', '제임스', '카터', '39번째']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mask된 단어들\n",
    "vocab.decode_ids(test_label[1][0].tolist()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2b3c3202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "<1번째>\n",
      "input_sentence: [CLS]전을 이끌고 새로운 분야들을 낳았다. 응용이[MASK] 수학 자체의an苦よ 북쪽 재미를 추구하며 연구하는 것을 순수수학이라 하는데, 긴 시간이 지난 뒤에 순수수학적 연구를 다른[MASK][MASK] 응용할 방법이 발견된 경우도 많았다고 한다.[SEP] 수학은 숫자[MASK][MASK][MASK] 계산, 측정[MASK] 물리적 대상의 모양과 움직임을 추상화하고, 이에 논리적 추론을 적용하여 나타났다. 이런 기본 개념들은 고대 이집트, 메소포타미아, 고대 인도, 고대 중국[MASK] 고대 그리스의 수학책에서[SEP]\n",
      "real NSP: 0\n",
      "real MASK: ['아닌', '아름다움과', '순수수학이라', '분야에', '세기,', '측정', '및', '및']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Predict NSP: 0\n",
      "Predict MASK: ['아닌', '순수학에서', '순수수학이라', '분야에', '수미,', '측정', '등의', '등']\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "<2번째>\n",
      "input_sentence: [CLS] 대한민국 제16대 대통령 선거[MASK] 제16대 대통령 선거는 2002년 12월 19일 목요일 한국의 새로운 대통령을 뽑기 위한 선거로[MASK][MASK][MASK][MASK][SEP] 16대 대선은 지난 15대 대선에서 간발의 차로 낙선하고 재도전한 이회창 한나라당 2, 찾아 사상 최초의 국민 참여 경선을 통해 여당의 대통령 후보가 된 노무현[MASK][MASK][MASK][MASK][MASK]시즌褒 양강 구도로 진행되었다. 대선 재수생인 이회창 후보는[MASK][MASK] 세력[MASK][MASK] 노무현 후보보다 대권 고지에 좀 더 유리할 것으로 점[SEP]\n",
      "real NSP: 1\n",
      "real MASK: ['대한민국', '치러졌다.', '후보와', '새천년민주당', '후보의', '경험이나', '면에서']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Predict NSP: 1\n",
      "Predict MASK: ['당시', '낙선하였다.', '후보대', '한회현현에서', '후보선에서', '노당의', '후보의']\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "<3번째>\n",
      "input_sentence: [CLS] 유리의[MASK][MASK] 금속의 정화로 이어지는 불로 인해 야금이 부상했다. 야금의 초기 단계에서[MASK][MASK][MASK] 정화 방법이 요구되었고, 금은 BC 2900[MASK] 초기의 고대 이집트의 귀중한 금속이되었다.[SEP][MASK][MASK] 통제 된 방식으로 사용 된 최초의 화학 반응은 불이였다. 그러나 천년 동안 불은 단순히 열과 빛을 생성하면서[MASK] 물질을 다른 물질 (타는[MASK] 또는 끓는 물)로 변형시킬[MASK][MASK] 신비한 힘으로만[MASK][MASK][MASK] 불은 초기 사회의 여러[SEP]\n",
      "real NSP: 0\n",
      "real MASK: ['발견과', '금속의', '년', '아마도', '한', '(타는', '나무', '수있는', '알려졌다.']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Predict NSP: 1\n",
      "Predict MASK: ['정은', '금금한', '년', '이집리의', '다른', '(타는', '물질', '때.', '작용했다.']\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "<4번째>\n",
      "input_sentence: [CLS][MASK][MASK][MASK] 존재는 레온하르트 오일러가 예상하였으나, 최초의 초월수는 1844년에 조제프[MASK][MASK][MASK][MASK] 발견하였다. 그는 초월수의 예로서 다음과 같이 정의되는 리우빌 상수를 제시하였다.[SEP] 초월수의 존재를 증명하기 위해 특별히 만들어진 수가[MASK] 수 중에서 처음으로 초월수임이 증명된 수는 상수 e로, 샤를 에르미트가 1873년에 증명하였다. 1882년에는[MASK][MASK][MASK][MASK] 폰 린데만이 원주율[MASK] 초월수임을 증명하였다.[MASK] 고대 그리스 시대부터의 난제였던[SEP]\n",
      "real NSP: 1\n",
      "real MASK: ['초월수의', '리우빌이', '예로서', '아닌', '샤를', '페르디난트', '또한', '이것은']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Predict NSP: 1\n",
      "Predict MASK: ['초월스의', '오린데를', '예로서', '많은', '샤를', '에르르트르트', '또는', '그는']\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "<5번째>\n",
      "input_sentence: [CLS] 12일 당 차원에서 특정 후보를 지지하지 않기로 선언하였다. 다만 당원 및 당직자들이 개별적인 지지를 하는[MASK] 막지[MASK][MASK] 해, 이인제는 다수 자민련 의원들과 함께 이회창 지지를 선언하고 이회창 후보 지원 활동에 나섰다.[SEP] 정몽준 후보는 단일화 여론조사[MASK][MASK] 패배함에 따라 사퇴를 선언하고 노무현 후보 지지를 선언하였다. 노무현[MASK][MASK] 정몽준 후보와의 단일화를 계기로 각종 여론조사에서 이회창[MASK][MASK] 2014互 많은 하지[SEP]\n",
      "real NSP: 0\n",
      "real MASK: ['것은', '않기로', '여론조사', '결과에서', '후보는', '각종', '후보를', '역전하였다.']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Predict NSP: 1\n",
      "Predict MASK: ['것을', '못게', '여론조사', '후보에', '후보와', '각종', '후보를', '후보이하였다.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ran_num = random.randint(0, 99)\n",
    "    print('-'*130)\n",
    "    print('<{}번째>'.format(i+1))\n",
    "    print('input_sentence:',vocab.decode_ids(test_input[0][ran_num].tolist()))\n",
    "    print('real NSP:', test_label[0][ran_num])\n",
    "    print('real MASK:', vocab.decode_ids(test_label[1][ran_num].tolist()).split())\n",
    "    print('-'*130)\n",
    "    outputs_nsp, outputs_mlm = pre_train_model((test_input[0][ran_num].reshape(1,-1), test_input[1][ran_num].reshape(1,-1)))\n",
    "    \n",
    "    predict_mask = [np.argmax(i) for i in outputs_mlm[0]]\n",
    "    predict_mask *= np.clip(test_label[1][ran_num], 0, 1) #np.clip는 test_label내 0보다 작은 값들을 0으로 바꿔주고 1보다 큰 값들을 1로 바꾼 후 곱해줌.\n",
    "    print('Predict NSP:', np.argmax(outputs_nsp)) #전체 배열에서 가장 높은 값을 가진 요소의 인덱스 배열을 반환 / 중요!!\n",
    "    print('Predict MASK:',vocab.decode_ids(predict_mask.tolist()).split())\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "753836b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 67ms/step - loss: 12.2201 - nsp_loss: 0.6311 - mlm_loss: 11.5890 - nsp_acc: 0.6500 - mlm_lm_acc: 0.2993\n"
     ]
    }
   ],
   "source": [
    "test_result = pre_train_model.evaluate(test_input, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c4c87",
   "metadata": {},
   "source": [
    "- 다음 문장이 이어지는지 맞추는 문제(NSP)는 65%의 정확도를 보이고, 마스킹된 글자를 맞추는 문제(MLM)는 29%의 정확도를 보였습니다.\n",
    "- 마스킹을 맞추는 문제가 좀 더 어려운 훈련이라고 보여집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc101ca7",
   "metadata": {},
   "source": [
    "# 전체 회고\n",
    "- 데이터를 직접 변형하여 사전학습을 처음 시켜보았습니다.\n",
    "- BERT 사전학습 코드를 확실하게 이해해야 할 것 같습니다.\n",
    "- 마스킹 문제의 경우 사전에 있는 단어의 수가 output이 되다 보니 성능이 더 낮게 나온것으로 생각됩니다.\n",
    "- 이번 노드에서는 크게 어려운것은 없었지만, 테스트 과정 중 np.clip(test_label[1][ran_num], 0, 1)에서 시간을 할애했던것 같습니다.\n",
    "- 저것을 해주지 않아 계속 원래의 input sentence가 반환되었지만, 선배 노드를 통해서 방법을 찾았습니다.\n",
    "\n",
    "- 만약 더 좋은 자원 환경에서 실험을 했다면, 제대로 된 사전학습 모델이 나왔을것으로 생각됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
